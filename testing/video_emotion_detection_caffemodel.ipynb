{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from statistics import mode\n",
    "from os.path import dirname, join\n",
    "from helper import get_labels\n",
    "from helper import detect_faces\n",
    "from helper import draw_text\n",
    "from helper import draw_bounding_box\n",
    "from helper import apply_offsets\n",
    "from helper import load_detection_model\n",
    "from helper import preprocess_input\n",
    "\n",
    "USE_WEBCAM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image detection\n",
    "# detection_model_path = '../models/detection/res10_300x300_ssd_iter_140000.caffemodel'\n",
    "# prototxt_path = join(dirname('.'), 'deploy.prototxt.txt')\n",
    "\n",
    "# video detection\n",
    "emotion_model_path = '../models/emotion/XCEPTION_107-0.66.hdf5'\n",
    "emotion_labels = get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters for bounding boxes shape\n",
    "frame_window = 10\n",
    "emotion_offsets = (20, 40)\n",
    "\n",
    "# loading models\n",
    "#net = cv2.dnn.readNetFromCaffe(prototxt_path, detection_model_path)\n",
    "net = cv.dnn.readNetFromCaffe('../models/detection/deploy.prototxt.txt', '../models/detection/res10_300x300_ssd_iter_140000.caffemodel')\n",
    "emotion_classifier = load_model(emotion_model_path, compile=False)\n",
    "\n",
    "# getting input model shapes for inference\n",
    "emotion_target_size = emotion_classifier.input_shape[1:3]\n",
    "\n",
    "# starting lists for calculating modes\n",
    "emotion_window = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream Started\n"
     ]
    }
   ],
   "source": [
    "# start video streaming\n",
    "cv2.namedWindow('window_frame')\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "print(\"Stream Started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dinner Video\n",
      "Loaded Video\n"
     ]
    }
   ],
   "source": [
    "cap = None\n",
    "if (USE_WEBCAM == True):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "else:\n",
    "    print(\"Loading Dinner Video\")\n",
    "    cap = cv2.VideoCapture('test_images/trailer.mp4')\n",
    "    \n",
    "print(\"Loaded Video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Frame\n",
      "Reading Frame\n",
      "Setting GRAY and RGB Images\n",
      "Set Blob as Modified Image\n",
      "Detecting Faces\n",
      "Iterating Over 165 Detections\n",
      "Reading Frame\n",
      "Setting GRAY and RGB Images\n",
      "Set Blob as Modified Image\n",
      "Detecting Faces\n",
      "Iterating Over 163 Detections\n",
      "Reading Frame\n",
      "Setting GRAY and RGB Images\n",
      "Set Blob as Modified Image\n",
      "Detecting Faces\n",
      "Iterating Over 163 Detections\n",
      "Reading Frame\n",
      "Setting GRAY and RGB Images\n",
      "Set Blob as Modified Image\n",
      "Detecting Faces\n",
      "Iterating Over 170 Detections\n",
      "Reading Frame\n",
      "Setting GRAY and RGB Images\n",
      "Set Blob as Modified Image\n",
      "Detecting Faces\n",
      "Iterating Over 171 Detections\n",
      "Reading Frame\n",
      "Setting GRAY and RGB Images\n",
      "Set Blob as Modified Image\n",
      "Detecting Faces\n",
      "Iterating Over 175 Detections\n",
      "Reading Frame\n",
      "Setting GRAY and RGB Images\n",
      "Set Blob as Modified Image\n",
      "Detecting Faces\n",
      "Iterating Over 170 Detections\n",
      "Reading Frame\n",
      "Setting GRAY and RGB Images\n",
      "Set Blob as Modified Image\n",
      "Detecting Faces\n",
      "Iterating Over 162 Detections\n",
      "Reading Frame\n",
      "Setting GRAY and RGB Images\n",
      "Set Blob as Modified Image\n",
      "Detecting Faces\n",
      "Iterating Over 168 Detections\n",
      "Reading Frame\n",
      "Setting GRAY and RGB Images\n",
      "Set Blob as Modified Image\n",
      "Detecting Faces\n",
      "Iterating Over 172 Detections\n",
      "Reading Frame\n",
      "Setting GRAY and RGB Images\n",
      "Set Blob as Modified Image\n",
      "Detecting Faces\n",
      "Iterating Over 178 Detections\n",
      "Reading Frame\n",
      "Setting GRAY and RGB Images\n",
      "Set Blob as Modified Image\n",
      "Detecting Faces\n",
      "Iterating Over 173 Detections\n",
      "Reading Frame\n",
      "Setting GRAY and RGB Images\n",
      "Set Blob as Modified Image\n",
      "Detecting Faces\n",
      "Iterating Over 170 Detections\n",
      "Reading Frame\n",
      "Setting GRAY and RGB Images\n",
      "Set Blob as Modified Image\n",
      "Detecting Faces\n",
      "Iterating Over 174 Detections\n",
      "Reading Frame\n",
      "Setting GRAY and RGB Images\n",
      "Set Blob as Modified Image\n",
      "Detecting Faces\n",
      "Iterating Over 153 Detections\n",
      "Reading Frame\n",
      "Setting GRAY and RGB Images\n",
      "Set Blob as Modified Image\n",
      "Detecting Faces\n",
      "Iterating Over 147 Detections\n",
      "Reading Frame\n",
      "Setting GRAY and RGB Images\n",
      "Set Blob as Modified Image\n",
      "Detecting Faces\n",
      "Iterating Over 162 Detections\n",
      "Reading Frame\n",
      "Setting GRAY and RGB Images\n",
      "Set Blob as Modified Image\n",
      "Detecting Faces\n",
      "Iterating Over 163 Detections\n",
      "Reading Frame\n",
      "Setting GRAY and RGB Images\n",
      "Set Blob as Modified Image\n",
      "Detecting Faces\n",
      "Iterating Over 156 Detections\n",
      "Reading Frame\n",
      "Setting GRAY and RGB Images\n",
      "Set Blob as Modified Image\n",
      "Detecting Faces\n",
      "Iterating Over 157 Detections\n",
      "Reading Frame\n",
      "Setting GRAY and RGB Images\n",
      "Set Blob as Modified Image\n",
      "Detecting Faces\n",
      "Iterating Over 153 Detections\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading Frame\")\n",
    "while cap.isOpened(): # True:\n",
    "    \n",
    "    print(\"Reading Frame\")\n",
    "    ret, bgr_image = cap.read()\n",
    "\n",
    "    #bgr_image = video_capture.read()[1]\n",
    "\n",
    "    print(\"Setting GRAY and RGB Images\")\n",
    "    gray_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)\n",
    "    rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    print(\"Set Blob as Modified Image\")\n",
    "    (h, w) = bgr_image.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(bgr_image, (300, 300)), 1.0,\n",
    "                                 (300, 300), (104.0, 177.0, 123.0))\n",
    "    \n",
    "    print(\"Detecting Faces\")\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    print(\"Iterating Over \" + str(detections.shape[2]) + \" Detections\")\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # extract the confidence (i.e., probability) associated with the prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        \n",
    "        # use detections passing a confidence threshold\n",
    "        if confidence > 0.240:\n",
    "            # calculate coordinates for detection box\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            \n",
    "            # calculate emotion predictions\n",
    "            try:\n",
    "                gray_face = cv2.resize(gray_face, (emotion_target_size)) # resize the image to emotion image size\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            gray_face = preprocess_input(gray_face, True) # convert pixel values to between -1 and 1\n",
    "            gray_face = np.expand_dims(gray_face, 0) # add space after each pixel coordinate\n",
    "            gray_face = np.expand_dims(gray_face, -1) # add new line after each pixel coordinate\n",
    "            emotion_prediction = emotion_rec.predict(gray_face) # use model to predict face emotion\n",
    "            emotion_probability = np.max(emotion_prediction) # assign the mostly like emotion values\n",
    "            emotion_label_arg = np.argmax(emotion_prediction) # assign most occuring emotion value\n",
    "            emotion_text = emotion_labels[emotion_label_arg] # # assign corresponding emotion text\n",
    "            emotion_window.append(emotion_text) # append the emotion text to array\n",
    "\n",
    "            if len(emotion_window) > frame_window: # if number of appended emotions is greater than 10\n",
    "                emotion_window.pop(0) # remove the oldest predicted emotion\n",
    "            try:\n",
    "                emotion_mode = mode(emotion_window) # create array of most commonly occuring emotions\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            # setup emotion colors\n",
    "            if emotion_text == 'angry':\n",
    "                color = emotion_probability * np.asarray((255, 0, 0))\n",
    "            elif emotion_text == 'sad':\n",
    "                color = emotion_probability * np.asarray((0, 0, 255))\n",
    "            elif emotion_text == 'happy':\n",
    "                color = emotion_probability * np.asarray((255, 255, 0))\n",
    "            elif emotion_text == 'surprise':\n",
    "                color = emotion_probability * np.asarray((0, 255, 255))\n",
    "            else:\n",
    "                color = emotion_probability * np.asarray((0, 255, 0))\n",
    "\n",
    "            color = color.astype(int)\n",
    "            color = color.tolist()\n",
    "\n",
    "            # draw box and confidence\n",
    "            text = \"{:.2f}%\".format(confidence * 100)\n",
    "            y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "            cv2.rectangle(bgr_image, (startX, startY), (endX, endY), (0, 0, 255), 2)\n",
    "            cv2.putText(bgr_image, text, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "            draw_text(box, rgb_image, emotion_mode, color, 0, -45, 1, 1)\n",
    "    \n",
    "    cv2.imshow('window_frame', bgr_image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
